{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import get_gpt_logprobs, tokenize_gpt\n",
    "from data_utils import save_to_json, load_from_json\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = 'gpt-3.5-turbo-0125'\n",
    "# model = 'gpt-4-turbo-2024-04-09'\n",
    "model = \"gpt-4o-2024-05-13\"\n",
    "\n",
    "model_name = (\n",
    "    \"gpt4o\" if \"gpt-4o\" in model else \"gpt4\" if \"gpt-4\" in model else \"gpt35\"\n",
    ")\n",
    "input = \"\"\"Your task: Produce a list that contains, in order, a one-word country, a one-word capital city of a country, a one-word US state, a one-word US state capital, and the surname of an American president.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data_utils import update_solutions_json\n",
    "# update_solutions_json()\n",
    "results = load_from_json(f\"results/{model_name}_results.json\")\n",
    "solutions = load_from_json(\"solutions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_results(model_name, number_of_results=100):\n",
    "    results = []\n",
    "    for i in range(number_of_results):\n",
    "        logprobs, message = get_gpt_logprobs(model, input, temperature=1)\n",
    "        results.append({'logprob_data': logprobs, 'message': message, 'tokens': tokenize_gpt(logprobs)})\n",
    "\n",
    "    existing_results = load_from_json(f'results/{model_name}_results.json')\n",
    "    existing_results.extend(results)\n",
    "    save_to_json(existing_results, 'results/{model_name}_results.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_map = {}\n",
    "prefix_map[''] = results[0]['logprob_data'][0]\n",
    "for result in results:\n",
    "    prefix = ''\n",
    "    for token, logprobs in zip(result['tokens'], result['logprob_data'][1:]):\n",
    "        prefix += token\n",
    "        prefix_map[prefix] = logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prefix_map(results):\n",
    "    prefix_map = {}\n",
    "    prefix_map[''] = results[0]['logprob_data'][0]\n",
    "    for result in results:\n",
    "        prefix = ''\n",
    "        for token, logprobs in zip(result['tokens'], result['logprob_data'][1:]):\n",
    "            prefix += token\n",
    "            if prefix == 'Canada  \\nAmman  \\nNevada  \\nAustin  \\nN':\n",
    "                print(prefix, logprobs)\n",
    "            prefix_map[prefix] = logprobs\n",
    "    \n",
    "    return prefix_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_word_chain(words):\n",
    "    \"\"\" Returns True if the words follow the last letter/first letter rule, else False \"\"\"\n",
    "    words = [word.lower() for word in words]\n",
    "    for i in range(len(words) - 1):\n",
    "        if words[i][-1] != words[i + 1][0]:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_letter_set(arr):\n",
    "    return set([s[0] for s in arr])\n",
    "\n",
    "def last_letter_set(arr):\n",
    "    return set([s[-1] for s in arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of correct chains: 24686\n",
      "Examples:\n",
      "[['russia', 'amsterdam', 'massachusetts', 'salem', 'monroe'],\n",
      " ['rwanda', 'amsterdam', 'massachusetts', 'salem', 'monroe'],\n",
      " ['samoa', 'amsterdam', 'massachusetts', 'salem', 'monroe'],\n",
      " ['serbia', 'amsterdam', 'massachusetts', 'salem', 'monroe'],\n",
      " ['slovakia', 'amsterdam', 'massachusetts', 'salem', 'monroe'],\n",
      " ['slovenia', 'amsterdam', 'massachusetts', 'salem', 'monroe'],\n",
      " ['somalia', 'amsterdam', 'massachusetts', 'salem', 'monroe'],\n",
      " ['syria', 'amsterdam', 'massachusetts', 'salem', 'monroe'],\n",
      " ['tanzania', 'amsterdam', 'massachusetts', 'salem', 'monroe'],\n",
      " ['tonga', 'amsterdam', 'massachusetts', 'salem', 'monroe']]\n",
      "Choosable countries: 152 / 182\n",
      "Choosable world capitals: 118 / 209\n",
      "Choosable states: 30 / 40\n",
      "Choosable state capitals: 14 / 41\n",
      "Choosable presidents: 12 / 39\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Print total number of correct solution chains and the number of choosable items for each category \"\"\"\n",
    "\n",
    "def build_last_letter_dict(strings):\n",
    "    prefix_dict = defaultdict(list)\n",
    "    for s in strings:\n",
    "        prefix_dict[s[-1]].append(s)\n",
    "    return prefix_dict\n",
    "\n",
    "last_letter_dict_countries = build_last_letter_dict(solutions['countries'])\n",
    "last_letter_dict_world_capitals = build_last_letter_dict(solutions['world_capitals'])\n",
    "last_letter_dict_states = build_last_letter_dict(solutions['states'])\n",
    "last_letter_dict_state_capitals = build_last_letter_dict(solutions['state_capitals'])\n",
    "\n",
    "correct_chains = []\n",
    "for p in solutions['presidents']:\n",
    "    for result in last_letter_dict_state_capitals[p[0]]:\n",
    "        for s in last_letter_dict_states[result[0]]:\n",
    "            for w in last_letter_dict_world_capitals[s[0]]:\n",
    "                for x in last_letter_dict_countries[w[0]]:\n",
    "                    correct_chains.append([x, w, s, result, p])\n",
    "\n",
    "print(f'Total number of correct chains: {len(correct_chains)}')\n",
    "print('Examples:')\n",
    "pprint(correct_chains[10000:10010])\n",
    "\n",
    "choosable_countries = set([c[0] for c in correct_chains])\n",
    "choosable_world_capitals = set([c[1] for c in correct_chains])\n",
    "choosable_states = set([c[2] for c in correct_chains])\n",
    "choosable_state_capitals = set([c[3] for c in correct_chains])\n",
    "choosable_presidents = set([c[4] for c in correct_chains])\n",
    "choosable = choosable_countries | choosable_world_capitals | choosable_states | choosable_state_capitals | choosable_presidents\n",
    "\n",
    "print(f'Choosable countries: {len(choosable_countries)} / {len(solutions[\"countries\"])}')\n",
    "print(f'Choosable world capitals: {len(choosable_world_capitals)} / {len(solutions[\"world_capitals\"])}')\n",
    "print(f'Choosable states: {len(choosable_states)} / {len(solutions[\"states\"])}')\n",
    "print(f'Choosable state capitals: {len(choosable_state_capitals)} / {len(solutions[\"state_capitals\"])}')\n",
    "print(f'Choosable presidents: {len(choosable_presidents)} / {len(solutions[\"presidents\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 64\n",
      "[]\n",
      "---\n",
      "1000 102\n",
      "['York', 'Denmark', 'Mexico', 'Yaound√©', 'Egypt', 'Nicaragua', 'Norway', 'Ecuador', 'Australia', 'Montreal', 'Ethiopia', 'Tangier', 'Yakarta', 'Kuwait', 'Yemen', 'November', 'Estonia', 'Addis', 'Yakima', 'England', 'ElKuwait', 'Uganda', 'Uruguay', 'Uzbekistan', 'Naples', 'Atlanta', 'Utah', 'Namaqua', 'Yakutsk', 'Lagos', 'Yak', 'Denver', 'Namibia', 'Youngstown', 'Edmonton', 'Kansas', 'Nigeria', 'Laos', 'La', 'Andorra', 'Nashville', 'Nepal', 'Elgin', 'Tallahassee', 'Austin', 'Lahore', 'New', 'Yangon']\n",
      "---\n",
      "1000 54\n",
      "['North', 'York', 'Seol', 'Southhold', 'South', 'Southhampton', 'Yemen', 'Harrisburg', 'Estonia', 'x-ray', 'Paz', 'Rhode', 'Delhi', 'Yale', 'X', 'Sofia', 'Everglades', 'Yellowstone', 'Xenia', 'New']\n",
      "---\n",
      "1000 63\n",
      "['Newmont', 'Exeter', 'Emporia', 'York', 'Santa', 'Omaha', 'Aspen', 'Eugene', 'Seattle', 'Edmondson', 'Nebraska', 'Illinois', 'Esmeralda', 'Kansas', 'Samoa', 'Texas', 'Nevada', 'Idaho', 'Yuma', 'Yakima', 'Orlando', 'Anchorage', 'Carolina', 'Evanston', 'Garfield', 'Dakota', 'Haven', 'Harrisoni', 'Arizona', 'Alaska', 'Lugosi', 'Kentucky', 'Everglades', 'Hawaii', 'Island', 'Eureka', 'Houston']\n",
      "---\n",
      "998 78\n",
      "['Sully', 'Springfield', 'Young', 'York', 'Sarkozy', 'Stanton', 'Seldon', 'Salem', 'Yard', 'Simpson', 'Seaton', 'Kentucky', 'Stover', 'Yancey', 'Sullivan', 'Yeltsin', 'Stinson', 'Yellen', 'Indianapolis', 'Sanders', 'Dixon', 'Stevens', 'Fe', 'Stout', 'Kane', 'Snipes', 'Snyder', 'Dodd', 'Norris', 'Atlanta', 'Yreka', 'Scott', 'Douglas', 'Albany', 'Icard', 'Yankee', 'Yates', 'Yancy', 'Stevenson', 'Dover', 'Yount', 'Denver', 'Singleton', 'Smith', 'Emerson', 'Harris', 'Samson', 'Stivers', 'Kansas', 'Seward', 'Yow', 'Yale', 'Davis', 'Sherman']\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Prints the invalid submitted tokens for each category\"\"\"\n",
    "\n",
    "countries = []\n",
    "capitals = []\n",
    "states = []\n",
    "state_capitals = []\n",
    "presidents = []\n",
    "submitted_items = [countries, capitals, states, state_capitals, presidents]\n",
    "for result in results:\n",
    "    words = result['message'].split()\n",
    "    for index in range(min(5, len(words))):\n",
    "        if index == 4:\n",
    "            presidents.append(words[index].replace('.', ''))\n",
    "        else:\n",
    "            submitted_items[index].append(words[index])\n",
    "    \n",
    "for name, l in [('countries', countries), ('world_capitals', capitals), ('states', states), ('state_capitals', state_capitals), ('presidents', presidents)]:\n",
    "    print(len(l), len(set(l)))\n",
    "    print([i for i in set(l) if i.lower() not in [j.lower() for j in solutions[name]]])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409 1000 0.409\n"
     ]
    }
   ],
   "source": [
    "score = 0\n",
    "valid_results = []\n",
    "for result in results:\n",
    "    try:\n",
    "        a, b, c, d, e = result['message'].split()[:5]\n",
    "        if a.lower() not in solutions['countries']:\n",
    "            continue\n",
    "        if b.lower() not in solutions['world_capitals']:\n",
    "            continue\n",
    "        if c.lower() not in solutions['states']:\n",
    "            continue\n",
    "        if d.lower() not in solutions['state_capitals']:\n",
    "            continue\n",
    "        if e.lower() not in solutions['presidents']:\n",
    "            continue\n",
    "        score += 1\n",
    "        valid_results.append(result)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(score, len(results), score / len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(341, 409, 0.8337408312958435)"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passing_results = []\n",
    "for result in valid_results:\n",
    "    words = result['message'].split()[:5]\n",
    "    if check_word_chain(words):\n",
    "        passing_results.append(result)\n",
    "\n",
    "len(passing_results), len(valid_results), len(passing_results) / len(valid_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canada  \n",
      "Amman  \n",
      "Nevada  \n",
      "Austin  \n",
      "N [['ixon', 0.9999993295729247], ['ix', 3.466323946336287e-07], ['ash', 6.82560337633487e-08], ['orris', 2.8453348089834e-08], ['olan', 2.8453348089834e-08], ['ixen', 2.510999155743982e-08], ['ixo', 2.215948977336598e-08], ['icol', 1.0467401794744658e-08], ['<|end|>', 1.0467401794744658e-08], ['aylor', 9.237449661970594e-09], ['icolas', 8.152020714470167e-09], ['orton', 5.602796437537268e-09], ['ielsen', 5.602796437537268e-09], ['oble', 4.363462252943702e-09], ['ox', 3.850741922767617e-09], ['issan', 3.850741922767617e-09], ['IX', 3.850741922767617e-09], ['ikon', 2.335593038799337e-09], ['ye', 1.8189616875530459e-09], ['ison', 1.8189616875530459e-09]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 341/341 [00:00<00:00, 17948.67it/s]\n"
     ]
    }
   ],
   "source": [
    "prefix_map = compute_prefix_map(results)\n",
    "\n",
    "total_prob_map = []\n",
    "for result in tqdm(passing_results):\n",
    "    # print(result['message'])\n",
    "    # print(result['tokens'])\n",
    "    response = ''\n",
    "    try:\n",
    "        total_prob = [i[1] for i in prefix_map[response] if i[0] == result['tokens'][0]][0]\n",
    "        # print(total_prob)\n",
    "    except:\n",
    "        total_prob = prefix_map[''][-1][1]\n",
    "    # print(result['tokens'][0], total_prob)\n",
    "    for token, next_token in zip(result['tokens'], result['tokens'][1:]):\n",
    "        response += token\n",
    "        try:\n",
    "            prob = [i[1] for i in prefix_map[response] if i[0] == next_token][0]\n",
    "        except:\n",
    "            try:\n",
    "                prob = prefix_map[response][-1][1]\n",
    "            except:\n",
    "                prob = 1\n",
    "        total_prob *= prob\n",
    "        # print(next_token, prob)\n",
    "    result['total_prob']= total_prob\n",
    "    # print('\\n', total_prob)\n",
    "\n",
    "save_to_json(results, f\"results/{model_name}_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_json([result['total_prob'] for result in passing_results], f'{model_name}_total_probs.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ixon', 0.9999993295729247],\n",
       " ['ix', 3.466323946336287e-07],\n",
       " ['ash', 6.82560337633487e-08],\n",
       " ['orris', 2.8453348089834e-08],\n",
       " ['olan', 2.8453348089834e-08],\n",
       " ['ixen', 2.510999155743982e-08],\n",
       " ['ixo', 2.215948977336598e-08],\n",
       " ['icol', 1.0467401794744658e-08],\n",
       " ['<|end|>', 1.0467401794744658e-08],\n",
       " ['aylor', 9.237449661970594e-09],\n",
       " ['icolas', 8.152020714470167e-09],\n",
       " ['orton', 5.602796437537268e-09],\n",
       " ['ielsen', 5.602796437537268e-09],\n",
       " ['oble', 4.363462252943702e-09],\n",
       " ['ox', 3.850741922767617e-09],\n",
       " ['issan', 3.850741922767617e-09],\n",
       " ['IX', 3.850741922767617e-09],\n",
       " ['ikon', 2.335593038799337e-09],\n",
       " ['ye', 1.8189616875530459e-09],\n",
       " ['ison', 1.8189616875530459e-09]]"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_map['Canada  \\nAmman  \\nNevada  \\nAustin  \\nN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
